{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54faddc4-cda4-4a20-925c-39ea7dc64f23",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Foundation\n",
    "This module is based on the DeepMind lectures given by David Silver, you can find the following material below:\n",
    "\n",
    "- [DeepMind RL Course](https://www.davidsilver.uk/teaching/)\n",
    "- [Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)\n",
    "\n",
    "#### Table of Contents\n",
    "1. Introduction (Basic Ideas behind RL)\n",
    "2. Markov Decision Processes\n",
    "3. Planning by Dyanmic Programming\n",
    "4. Model-Free Prediction\n",
    "5. Model-Free Control\n",
    "6. Value Functon Approximation\n",
    "7. Policy Gradient Methods\n",
    "8. Integrating Learning and Planning\n",
    "9. Exploration and Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f48fa2-0df5-49be-b740-1ee36889f55e",
   "metadata": {},
   "source": [
    "### A Brief Introduction\n",
    "\n",
    "Reinforcement is the third category of machine learning, completely seperate from supervised and unsupervised learning.\n",
    "\n",
    "**Sequential Decision Making**\n",
    "The goal of the agent is to pick a sequence of actions that maximizes the largest future reward. Actions are not primarily short and can be long term.\n",
    "\n",
    "At each time step (t) and agent and environment:\n",
    "1. Executes an action A<sub>t</sub>\n",
    "2. Recieves an observation O<sub>t</sub>\n",
    "3. Recieves a reward R<sub>t</sub>\n",
    "\n",
    "**History and State**\n",
    "\n",
    "The history is a sequence of all prior observations, actions, and rewards.\n",
    "\n",
    "H<sub>t</sub> = O<sub>t</sub>, A<sub>t</sub>, R<sub>t</sub>\n",
    "\n",
    "#### Definitions of State\n",
    "**Environment State (S<sub>t</sub><sup>e</sup>)**\n",
    "\n",
    "The rules that make up the environment (e.g. physics and differential equations that exmplain the physical world, rules in atari)\n",
    "\n",
    "**Not visible by the agent, thus the agent must make an observation action off their observation\n",
    "\n",
    "\n",
    "**Agent State (A<sub>t</sub><sup>a</sup>)**\n",
    "\n",
    "The agents internal representation of the world. The agent state is used to pick the actions that leads to the largest cumulative reward\n",
    "\n",
    "**Information State (Markov Decision Process)**\n",
    "\n",
    "Contains all usefull infromation from the history.\n",
    "\n",
    "Can be expressed as:\n",
    "\n",
    "P(S<sub>t+1</sub> | S<sub>t</sub>) = P(S<sub>t+1</sub> | S<sub>1</sub>, S<sub>2</sub>, S<sub>3</sub>...)\n",
    "\n",
    "*The current state in an markov decision process is equivalent to all previous states, thus we can ignore all previous states and only focus on the last*\n",
    "\n",
    "#### Types of Environments\n",
    "\n",
    "**Fully Observable**\n",
    "\n",
    "The agent direcly observes the environment and understands all aspects of the system\n",
    "\n",
    "O<sub>t</sub> = S<sub>t</sub><sup>a</sup> = S<sub>t</sub><sup>e</sup>\n",
    "\n",
    "The observation is equal to the state of the agent which is equal to the state of the environment\n",
    "\n",
    "**Partially Observable**\n",
    "\n",
    "The agent indirectly observes the environment\n",
    "\n",
    "For example the camera in a robot does not understand it's absolute location relative to the environment thus it has to explore the area to create an understanding\n",
    "\n",
    "\n",
    "#### Architecture of Agents\n",
    "\n",
    "**Policy:** The agents behavior. Is a map from the a state to an action. A policy can either be:\n",
    "\n",
    "1. Deterministic: a = pi(s)\n",
    "2. Stochastic: pi(a|s) = P[A=a, S=s]\n",
    "\n",
    "**Value Function:** How good is each state or action? Is a prediction of the expected future reward and is used to evaluate the goodness/badness of states\n",
    "\n",
    "V<sub>pi</sub> = Expected value<sub>pi</sub>[R<sub>t</sub> + gamma<sup>1</sup>R<sub>t+1</sub> + gamma<sup>2</sup>R<sub>t+1</sub>... | S<sub>t</sub>=s)\n",
    "\n",
    "where gamma<sup>n</sup> is a decay function (we care more about immediate actions and states)\n",
    "\n",
    "**Model:** The agent's representation of the environment. Predicts what the environment will do next. \n",
    "\n",
    "1. Transitions: P predicts next state (e.g. dynamics)\n",
    "2. Rewards: R predicts next immediate reward\n",
    "\n",
    "A dumb way to think about it: \"If we take this transition, what is the reward?\"\n",
    "\n",
    "#### Categories of Agents\n",
    "\n",
    "**Value based:** value function, no policy(implicit)\n",
    "\n",
    "**Policy based:** policy, no value function\n",
    "\n",
    "**Actor Critic:** value and policy based\n",
    "\n",
    "**Model Free:** policy and/or value function (in this scenario we do not try to understand the environment state)\n",
    "\n",
    "**Model Based:** model of environment\n",
    "\n",
    "#### General Problems with RL\n",
    "\n",
    "Reinforcement learning falls under two subsequent categories:\n",
    "\n",
    "- **RL:** The environment is initially unknown and we must interact with it through trial and error runs to gain an understaning.\n",
    "- **Planning:** We already have a full understanding and model of the environment; instead of interacting with the environment the agent preforms computations with its model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35c417-8c5d-4f4b-a21f-4a9d78db8275",
   "metadata": {},
   "source": [
    "### Markov Decision Processes\n",
    "\n",
    "A Markov Decision Process essentially says that all prior steps can be summarized by the current step; in short the current step is dependent on all prior steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9877f-fef5-4742-91e2-3d7c00fb026a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0955b0d-ed2f-42cd-a20d-226773d831c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
