{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f30b68b-eeaf-4210-832f-bc09b0177513",
   "metadata": {},
   "source": [
    "### Universal Manipulation Interface\n",
    "- [Website](https://umi-gripper.github.io/)\n",
    "- [Paper](https://arxiv.org/pdf/2402.10329.pdf)\n",
    "\n",
    "### How it works\n",
    "\n",
    "#### Overall Problem with Data Collection\n",
    "- Teleportation systems are expensive to setup and require expoert and precise operation. \n",
    "- Humand video is too messy and requires too large of an inference gap from video to robot embodiement (this can be solved)\n",
    "\n",
    "#### Problem with Gripper Collection\n",
    "- Motion blur with reduces action precision\n",
    "- Too small of a context window around env\n",
    "- Latency discrepancy: none during collection but some during inference\n",
    "\n",
    "#### Failure Cases\n",
    "- Object not in FOV\n",
    "- Object exceeded joint limit\n",
    "- Absolute action\n",
    "- Not latency matching\n",
    "- Incorrect classification \n",
    "\n",
    "#### Setup\n",
    "- GoPro\n",
    "- IMU sensor (using that in GoPro)\n",
    "- Gripper\n",
    "- Side Mirrors (adds a sense of depth)\n",
    "\n",
    "#### New Terms\n",
    "- Monocular structure-from-motion (SfM) -> recovers robot actions\n",
    "- Visuomotor Policy\n",
    "- Prehensile (Grip/Grab) and non-prehensile (Push/Move)\n",
    "- Proprioception (your body's ability to sense movement, action, and location). Tells your body where you are in space and time. Example: close your eyes and touch your index finger to your nose\n",
    "\n",
    "#### New Insights\n",
    "- Take observations (RGB images, dof, dimensions of controller (gripper and product a sequence of actions that acheive the set out goal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7cd455-e089-4bff-ac7c-a5ae46f48830",
   "metadata": {},
   "source": [
    "### DexCap\n",
    "- [Website](https://dex-cap.github.io/)\n",
    "- [Paper](https://arxiv.org/pdf/2403.07788.pdf)\n",
    "- [Code](https://github.com/j96w/DexCap)\n",
    "\n",
    "#### Problem\n",
    "- How to scale up training data for imitation learning? Extremely timely and costly\n",
    "- Videos of humans preforming tasks in in 2D, which fails to caputre 3D environments, thus requiring additional data to bridge the gap\n",
    "\n",
    "#### Solution\n",
    "- Capture detailed finger motion\n",
    "- Accurate 6-DoF wrist post estimation\n",
    "- Aligned 3D observations from recording with coordinate frame of hands\n",
    "- Protability and ease of use for data collection\n",
    "\n",
    "#### Setup\n",
    "- Intel Realsense T265 camera (fisheye of course) and L515 RGB-D LiDAR camera\n",
    "- IMU sensor\n",
    "- Electromagnetic conductive material\n",
    "- SLAM algo\n",
    "\n",
    "#### New Terms\n",
    "- In-hand  object re-orientation\n",
    "- Mocap (motion capture)\n",
    "- Electromagnetic Gloves (conductive material that measures different electricial potentials for example the change in potentials on the skin)\n",
    "- RGB-D (red, green, blue, depth; lidar works by shining a laser extremely fast around an environment, it then measures the time the laser takes to travel to and from an object in the environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc3985-6315-4dcf-92ca-fcfca9869d17",
   "metadata": {},
   "source": [
    "### Human Pose Estimatation: A Survey\n",
    "- [Paper](https://arxiv.org/pdf/2308.13872.pdf)\n",
    "\n",
    "\n",
    "#### Problems\n",
    "- There is a vast amount of different poses which depend on the shape and body of the person, what they are wearing, lighting, and environment, etc.\n",
    "- Occlusion which occurs when an object is blocked by something else. For example if I have a camera and want to predict the landmarks of my finger, yet there there is a cup in front of a few of my fingers, then it will be extremely hard to predict the landmarks for the occluded fingers\n",
    "\n",
    "#### Reading Insights\n",
    "- Method for HPE using deep learning include using the following: CNNs, RNN, GCNs.\n",
    "- CNNs are made up of two parts; 1. using some pretrained model such as Alexnet or Resnet on a large dataset such as Imagenet, and then ontop of this classification layer adding HPE pretrained models such a Hourglass, Cascaded Pyramid Network (CPN), or HRNet.\n",
    "- Prediction heads estimate human poses, two main methods are used: direcly predict joint coordinates using some regression method, or apply a heatmap over the classified image and then apply some regression method\n",
    "- Graph Convolutional Network (GCN) takes the graph of a pose as the input instead of an image as the pose of a humand can be represetned as a graph, or the magnitudes between different landmarks. It would then be intersting to understand how you can estimate a pose based on its temporal information when preforminga task such as lifting a weight. This is quite interesting because you are understanding a pose not based on how it looks visually, but based of the coordinate spacing of all the predefined landmarks, thus the definition of a pose is something that is much different than the way we understand a pose.\n",
    "\n",
    "\n",
    "\n",
    "#### New Terms\n",
    "- Bibliometrics (the statistical analysis of of books, articles, and other resources)\n",
    "- Occlusion (when something is blocked or hidden from the prominent view)\n",
    "- Temporal Information (describes how something behaves or evolves over time, eg. signal processing)\n",
    "- âˆ€ (\"for all\" or \"for every\", is used to indicate that a particular statement holds true for given set or domain)\n",
    "- HPE Datasets (COCO, MPII, CrowdPose, and PoseTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840cf5c7-fb0a-4606-b0f9-c1fb91eb6b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
